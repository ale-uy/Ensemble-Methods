{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Ensembles of Weak Learners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/sequential.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WEAK LEARNERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si bien la definición precisa de la fortaleza de los alumnos (learners) se basa en la teoría del aprendizaje automático, para nuestros propósitos, un alumno fuerte es un buen modelo (o estimador). Por el contrario, un alumno débil (weak learner) es un modelo muy simple que no funciona tan bien. El único requisito de un alumno débil (para la clasificación binaria) es que se desempeñe mejor que las conjeturas aleatorias. \n",
    "Dicho de otra manera, su precisión debe ser solo un poco mejor que el 50%. Los árboles de decisión se utilizan a menudo como estimadores base para conjuntos secuenciales. Los algoritmos de impulso suelen utilizar tocones de decisión o árboles de decisión de profundidad 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost: Adaptive boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost es un algoritmo adaptativo: en cada iteración, entrena un nuevo estimador base que corrige los errores cometidos por el estimador base anterior. Por lo tanto, necesita alguna forma de garantizar que el algoritmo de aprendizaje base priorice los ejemplos de entrenamiento mal clasificados. AdaBoost hace esto manteniendo pesos sobre ejemplos de entrenamiento individuales. Intuitivamente, los pesos reflejan la importancia relativa de los ejemplos de entrenamiento. Los ejemplos clasificados incorrectamente tienen pesos más altos, mientras que los ejemplos clasificados correctamente tienen pesos más bajos. Cuando entrenamos el siguiente estimador base secuencialmente, los pesos permitirán que el algoritmo de aprendizaje priorice (y con suerte corrija) los errores de la iteración anterior. Este es el componente adaptativo de AdaBoost, que en última instancia conduce a un conjunto poderoso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training an ensemble of weak learners using AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def fit_boosting(X, y, n_estimators=10):\n",
    "    n_samples, n_features = X.shape\n",
    "    D = np.ones((n_samples, )) # Nonnegative weights, initialized to 1\n",
    "    estimators = []\n",
    "    for t in range(n_estimators):\n",
    "        D = D / np.sum(D) # Normalizes the weights so they sum to 1\n",
    "        h = DecisionTreeClassifier(max_depth=1)\n",
    "        h.fit(X, y, sample_weight=D) # Trains a weak learner (h_t) with weighted examples\n",
    "        ypred = h.predict(X)\n",
    "        # Computes the training error (Epsilon_t) andthe weight (Epsilon_t) of the weak learner\n",
    "        e = 1 - accuracy_score(y, ypred, sample_weight=D) \n",
    "        a = 0.5 * np.log((1 - e) / e)\n",
    "        # Updates the example weights: increase for misclassified examples, decrease for correctly classified examples\n",
    "        m = (y == ypred) * 1 + (y != ypred) * -1\n",
    "        D *= np.exp(-a * m)\n",
    "        estimators.append((a, h)) # Saves the weak learner and its weight\n",
    "    return estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making predictions with AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_boosting(X, estimators):\n",
    "    pred = np.zeros((X.shape[0], )) # Initializes all the predictions to 0\n",
    "    for a, h in estimators:\n",
    "        pred += a * h.predict(X) # Makes weighted prediction for each example\n",
    "    y = np.sign(pred) # Converts weighted predictions to –1/1 labels\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Generates a synthetic classification data set of 200 points\n",
    "X, y = make_moons(n_samples=200, noise=0.1, random_state=13)\n",
    "\n",
    "y = (2 * y) - 1 # Converts 0/1 labels to –1/1 labels\n",
    "\n",
    "# Splits into training and test sets\n",
    "Xtrn, Xtst, ytrn, ytst = train_test_split(X, y, test_size=0.25, random_state=13)\n",
    "\n",
    "estimators = fit_boosting(Xtrn, ytrn) # Trains an AdaBoost model\n",
    "\n",
    "ypred = predict_boosting(Xtst, estimators) # Makes predictions with this AdaBoost\n",
    "\n",
    "# Podemos calcular la precisión general del conjunto de pruebas de nuestro modelo\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "tst_err = 1 - accuracy_score(ytst, ypred)\n",
    "\n",
    "print(tst_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost with scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "Xtrn, Xtst, ytrn, ytst = train_test_split(X, y, test_size=0.25, random_state=13)\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "shallow_tree = DecisionTreeClassifier(max_depth=2)\n",
    "ensemble = AdaBoostClassifier(\n",
    "    base_estimator=shallow_tree, # The base-learning algorithm AdaBoost uses to train weak learners\n",
    "    n_estimators=20, # The number of weak learners that will be trained sequentially by AdaBoost.\n",
    "    learning_rate=0.75 # An additional parameter that progressively shrinks the contribution of each successive weak learner trained for the ensemble.\n",
    "    )\n",
    "ensemble.fit(Xtrn, ytrn)\n",
    "\n",
    "ypred = ensemble.predict(Xtst)\n",
    "err = 1 - accuracy_score(ytst, ypred)\n",
    "print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost with scikit-learn (MULTICLASS CLASSIFICATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learn contiene la implementación multiclase de AdaBoost llamada Stagewise Additive Modeling usando pérdida exponencial multiclase, o SAMME. \n",
    "SAMME es una generalización del algoritmo de refuerzo adaptativo de Freund y Schapire de dos a múltiples clases. \n",
    "Además de SAMME, AdaBoostClassifier también ofrece una variante denominada SAMME.R. \n",
    "La diferencia clave entre estos dos algoritmos es que SAMME.R maneja predicciones de valor real de algoritmos de estimación base (es decir, probabilidades de clase), mientras que Vanilla SAMME maneja predicciones discretas (es decir, etiquetas de clase)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "Xtrn, Xtst, ytrn, ytst = train_test_split(X, y, test_size=0.25, random_state=13)\n",
    "ensemble = AdaBoostClassifier(base_estimator=shallow_tree, \n",
    "                              n_estimators=20, \n",
    "                              learning_rate=0.75, algorithm='SAMME.R')\n",
    "ensemble.fit(Xtrn, ytrn)\n",
    "ypred = ensemble.predict(Xtst)\n",
    "err = 1 - accuracy_score(ytst, ypred)\n",
    "print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Cross validation to select the best *LEARNING RATE*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "n_learning_rate_steps, n_folds = 10, 10 \n",
    "# Sets up stratified 10-fold CV and initializes the search space\n",
    "learning_rates = np.linspace(0.1, 1.0, num=n_learning_rate_steps)\n",
    "splitter = StratifiedKFold(n_splits=n_folds, shuffle=True)\n",
    "trn_err = np.zeros((n_learning_rate_steps, n_folds))\n",
    "val_err = np.zeros((n_learning_rate_steps, n_folds))\n",
    "stump = DecisionTreeClassifier(max_depth=1) # Uses decision stumps as weak learners\n",
    "for i, rate in enumerate(learning_rates): # For all choices of learning rates\n",
    "    for j, (trn, val) in enumerate(splitter.split(X, y)): # For training, validation sets\n",
    "        model = AdaBoostClassifier(algorithm='SAMME', base_estimator=stump, n_estimators=10, learning_rate=rate)\n",
    "        model.fit(X[trn, :], y[trn]) # Fits a model to training data in this fold\n",
    "        trn_err[i, j] = 1 - accuracy_score(y[trn], model.predict(X[trn, :])) # Computes training and validation /\n",
    "        val_err[i, j] = 1 - accuracy_score(y[val], model.predict(X[val, :])) # errors for this fold\n",
    "trn_err = np.mean(trn_err, axis=1) # Averages training and validation /\n",
    "val_err = np.mean(val_err, axis=1) # errors across the folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Cross validation to select the *best number of weak learners*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimator_steps, n_folds = 5, 10\n",
    "# Sets up stratified 10-fold CV and initializes the search space\n",
    "number_of_stumps = np.arange(5, 50, n_estimator_steps)\n",
    "splitter = StratifiedKFold(n_splits=n_folds, shuffle=True)\n",
    "trn_err = np.zeros((len(number_of_stumps), n_folds))\n",
    "val_err = np.zeros((len(number_of_stumps), n_folds))\n",
    "stump = DecisionTreeClassifier(max_depth=1) # Uses decision stumps as weak learners\n",
    "for i, n_stumps in enumerate(number_of_stumps): # For all estimator sizes\n",
    "    for j, (trn, val) in enumerate(splitter.split(X, y)): # For training, validation sets\n",
    "        model = AdaBoostClassifier(algorithm='SAMME', \n",
    "                                   base_estimator=stump, \n",
    "                                   n_estimators=n_stumps, \n",
    "                                   learning_rate=1.0)\n",
    "        model.fit(X[trn, :], y[trn]) # Fits a model to training data in this fold\n",
    "        trn_err[i, j] = 1 - accuracy_score(y[trn], model.predict(X[trn, :])) # Computes the training and validation /\n",
    "        val_err[i, j] = 1 - accuracy_score(y[val], model.predict(X[val, :])) # errors for this fold\n",
    "trn_err = np.mean(trn_err, axis=1)\n",
    "val_err = np.mean(val_err, axis=1) # Averages the errors across the folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LogitBoost: Boosting with the logistic loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diferencias respecto a AdaBoost:\n",
    "* *Función de pérdida:* AdaBoost utiliza la función de pérdida exponencial, que se enfoca en corregir los ejemplos mal clasificados en cada iteración. Por otro lado, LogitBoost utiliza una función de pérdida logística, que se basa en el modelo logit y tiene como objetivo ajustar los pesos de los ejemplos de acuerdo con las probabilidades estimadas.\n",
    "* *Base estimator:* AdaBoost puede utilizar cualquier algoritmo de aprendizaje automático como base estimator, mientras que LogitBoost utiliza modelos de regresión logística como base estimator en cada iteración. Esto significa que LogitBoost está diseñado específicamente para problemas de clasificación binaria.\n",
    "* *Predicciones:* AdaBoost utiliza la suma ponderada de las predicciones de todos los clasificadores débiles para generar la predicción final. En contraste, LogitBoost utiliza las probabilidades estimadas por los clasificadores débiles y las combina mediante una regresión logística para obtener la predicción final.\n",
    "* *Interpretación probabilística:* LogitBoost proporciona una interpretación probabilística directa de las predicciones, ya que utiliza una función de pérdida logística y estima probabilidades. En cambio, AdaBoost no proporciona directamente una interpretación probabilística, ya que se basa en la función de pérdida exponencial.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LogitBoost for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.special import expit\n",
    "\n",
    "\n",
    "def fit_logitboosting(X, y, n_estimators=10):\n",
    "    n_samples, n_features = X.shape\n",
    "    D = np.ones((n_samples, )) / n_samples\n",
    "    p = np.full((n_samples, ), 0.5) # Initializes example weights, “pred” probabilities\n",
    "    estimators = []\n",
    "    for t in range(n_estimators):\n",
    "        z = (y - p) / (p * (1 - p)) # Computes working responses\n",
    "        D = p * (1 - p) # Computes new example weights\n",
    "        h = DecisionTreeRegressor(max_depth=1) # Use decision-tree regression as base estimators for classification problems\n",
    "        h.fit(X, z, sample_weight=D)\n",
    "        estimators.append(h) # Appends weak learners to ensemble Ft+1(x) = Ft(x) + ht(x)\n",
    "        if t == 0:\n",
    "            margin = np.array([h.predict(X) for h in estimators]).reshape(-1, )\n",
    "        else:\n",
    "            margin = np.sum(np.array([h.predict(X) for h in estimators]), axis=0)\n",
    "        p = expit(margin) # Updates prediction probabilities\n",
    "    return estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LogitBoost for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_logit_boosting(X, estimators):\n",
    "    pred = np.zeros((X.shape[0], ))\n",
    "    for h in estimators:\n",
    "        pred += h.predict(X)\n",
    "    y = (np.sign(pred) + 1) / 2 # Converts –1/1 predictions to 0/1\n",
    "    return y"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
