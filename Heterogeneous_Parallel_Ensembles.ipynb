{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los conjuntos heterogéneos se pueden dividir en dos toipos, dependiendo de cómo combinen las predicciones individuales del estimador base en una predicción final:\n",
    "\n",
    "* **Weighting methods**: Estos métodos asignan a las predicciones individuales del estimador base un peso que corresponde a su fuerza. A los mejores estimadores base se les asignan pesos más altos e influyen más en la predicción final general. Las predicciones de los estimadores de base individuales se introducen en una función de combinación predeterminada, que hace las predicciones finales.\n",
    "\n",
    "* **Meta-learning methods**: Estos métodos usan un algoritmo de aprendizaje para combinar las predicciones de los estimadores base; las predicciones de los estimadores de base individuales se tratan como metadatos y se pasan a un meta-aprendiz de segundo nivel, que está capacitado para hacer predicciones finales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base estimators for heterogeneous ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X, y = make_moons(600, noise=0.25, random_state=13)\n",
    "\n",
    "X, Xval, y, yval = train_test_split(X, y, test_size=0.25) # Sets aside 25% of the data for validation\n",
    "\n",
    "Xtrn, Xtst, ytrn, ytst = train_test_split(X, y, test_size=0.25) # Sets aside a further 25% of the data for hold-out testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting different base estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "estimators = [\n",
    "    ('dt', DecisionTreeClassifier (max_depth=5)),\n",
    "    ('svm', SVC(gamma=1.0, C=1.0, probability=True)),\n",
    "    ('gp', GaussianProcessClassifier(RBF(1.0))),\n",
    "    ('3nn', KNeighborsClassifier(n_neighbors=3)),\n",
    "    ('rf',RandomForestClassifier(max_depth=3, n_estimators=25)),\n",
    "    ('gnb', GaussianNB())]\n",
    "\n",
    "def fit(estimators, X, y):\n",
    "    for model, estimator in estimators:\n",
    "        # Fits base estimators on the training data using these different learning algorithms\n",
    "        estimator.fit(X, y)\n",
    "    return estimators\n",
    "\n",
    "estimators = fit(estimators, Xtrn, ytrn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Individual predictions of base estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# The flag “proba” allows us to predict labels or probability over the labels.\n",
    "def predict_individual(X, estimators, proba=False):\n",
    "    n_estimators = len(estimators)\n",
    "    n_samples = X.shape[0]\n",
    "    y = np.zeros((n_samples, n_estimators))\n",
    "    for i, (model, estimator) in enumerate(estimators):\n",
    "        if proba:\n",
    "            # If true, predicts the probability of Class 1 (returns a float point probability value between 0 and 1)\n",
    "            y[:, i] = estimator.predict_proba(X)[:, 1]\n",
    "        else:\n",
    "            # Otherwise, directly predicts Class 1 (returns an integer class label 0 or 1)\n",
    "            y[:, i] = estimator.predict(X) \n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Majority Vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining predictions using majority vote\n",
    "from scipy.stats import mode\n",
    "\n",
    "\n",
    "def combine_using_majority_vote(X, estimators):\n",
    "    y_individual = predict_individual(X, estimators, proba=False)\n",
    "    y_final = mode(y_individual, axis=1, keepdims=False)\n",
    "    # Reshapes the vector to ensure it returns one prediction per example\n",
    "    return y_final[0].reshape(-1, )\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "ypred = combine_using_majority_vote(Xtst, estimators)\n",
    "tst_err = 1 - accuracy_score(ytst, ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining using **Accuracy Weighting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_using_accuracy_weighting(X, estimators, Xval, yval): # Takes the validation set as input\n",
    "    n_estimators = len(estimators)\n",
    "    # Gets individual predictions on the validation set\n",
    "    yval_individual = predict_individual(Xval, estimators, proba=False)\n",
    "    # Sets the weight for each base classifier as its accuracy score\n",
    "    wts = [accuracy_score(yval, yval_individual[:, i]) for i in range(n_estimators)]\n",
    "    wts /= np.sum(wts) # Normalizes the weights\n",
    "    ypred_individual = predict_individual(X, estimators, proba=False)\n",
    "    y_final = np.dot(ypred_individual, wts) # Computes the weighted combination of individual labels efficiently\n",
    "    return np.round(y_final) # Converts the combined prediction into a 0–1 label by rounding\n",
    "\n",
    "ypred = combine_using_accuracy_weighting(Xtst, estimators, Xval, yval)\n",
    "tst_err = 1 - accuracy_score(ytst, ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy weighting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La entropía, o entropía de la información para ser precisos, fue originalmente ideada por Claude Shannon para cuantificar la “cantidad de información” transmitida por una variable. Esto está determinado por dos factores: (1) el número de valores distintos que puede tomar la variable y (2) la incertidumbre asociada con cada valor.\n",
    "\n",
    "Considere que tres pacientes, Ana, Bob y Cam, están en el consultorio del médico esperando el diagnóstico de una enfermedad por parte del médico. A Ana se le dice con un 90 % de confianza que está sana (es decir, un 10 % de probabilidad de que esté enferma). A Bob se le dice con un 95 % de confianza que está enfermo (es decir, un 5 % de posibilidades de que esté sano). A Cam se le dice que los resultados de su prueba no son concluyentes (es decir, 50%/50%).\n",
    "Ana ha recibido buenas noticias y hay poca incertidumbre en su diagnóstico. Aunque Bob ha recibido malas noticias, también hay poca incertidumbre en su diagnóstico.\n",
    "\n",
    "La situación de Cam es de máxima incertidumbre: no ha recibido ni buenas ni malas noticias y le esperan más pruebas.\n",
    "La entropía cuantifica esta noción de incertidumbre a través de varios resultados. Las medidas basadas en la entropía se usan comúnmente durante el aprendizaje del árbol de decisiones para identificar con avidez las mejores variables para dividir y se usan como funciones de pérdida en redes neuronales profundas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(y):\n",
    "    _, counts = np.unique(y, return_counts=True)\n",
    "    p = np.array(counts.astype('float') / len(y))\n",
    "    ent = -p.T @ np.log2(p)\n",
    "    return ent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay dos diferencias clave entre la ponderación de entropía y la ponderación de precisión:\n",
    "\n",
    "* La precisión de un clasificador base se calcula usando las etiquetas verdaderas ytrue y las etiquetas predichas ypred. De esta manera, la métrica de precisión mide qué tan bien se desempeña un clasificador. Un clasificador con alta precisión es mejor.\n",
    "* La entropía de un clasificador base se calcula utilizando solo las etiquetas predichas ypred, y la métrica de entropía mide la incertidumbre de un clasificador sobre sus predicciones. Un clasificador con baja entropía (incertidumbre) es mejor. Por lo tanto, los pesos de los clasificadores base individuales son inversamente proporcionales a sus correspondientes entropías."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining using entropy weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_using_entropy_weighting(X, estimators, Xval): # Takes only the validation examples\n",
    "    n_estimators = len(estimators)\n",
    "    # Gets individual predictions on the validation set\n",
    "    yval_individual = predict_individual(Xval, estimators, proba=False) \n",
    "    # Sets the weight for each base classifier as its inverse entropy\n",
    "    wts = [1/entropy(yval_individual[:, i]) for i in range(n_estimators)]\n",
    "    wts /= np.sum(wts) # Normalizes the weights\n",
    "    ypred_individual = predict_individual(X, estimators, proba=False)\n",
    "    y_final = np.dot(ypred_individual, wts) # Computes the weighted combination of individual labels efficiently\n",
    "    return np.round(y_final) # Returns the rounded predictions\n",
    "\n",
    "ypred = combine_using_entropy_weighting(Xtst, estimators, Xval)\n",
    "tst_err = 1 - accuracy_score(ytst, ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dempster-Shafer combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_using_Dempster_Schafer(X, estimators):\n",
    "    p_individual = predict_individual(X, estimators, proba=True) # Gets individual predictions on the validation set\n",
    "    bpa0 = 1.0 - np.prod(p_individual, axis=1)\n",
    "    bpa1 = 1.0 - np.prod(1 - p_individual, axis=1)\n",
    "    # Stacks the beliefs for Class 0 and Class 1 side by side for every test example\n",
    "    belief = np.vstack([bpa0 / (1 - bpa0), bpa1 / (1 - bpa1)]).T\n",
    "    y_final = np.argmax(belief, axis=1) # Selects the final label as the class with the highest belief\n",
    "    return y_final\n",
    "\n",
    "ypred = combine_using_Dempster_Schafer(Xtst, estimators)\n",
    "tst_err = 1 - accuracy_score(ytst, ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining predictions by meta-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking\n",
    "El apilamiento es el método de meta-aprendizaje más común y recibe su nombre porque\n",
    "apila un segundo clasificador encima de sus estimadores base. El procedimiento general de apilamiento\n",
    "tiene dos pasos:\n",
    "* Nivel 1: Ajustar estimadores base sobre los datos de entrenamiento. Este paso es el mismo que antes.\n",
    "y tiene como objetivo crear un conjunto diverso y heterogéneo de clasificadores básicos.\n",
    "\n",
    "* Nivel 2: Construir un nuevo conjunto de datos a partir de las predicciones de los clasificadores base,\n",
    "que se convierten en meta-características. Las meta-características pueden ser las predicciones o las\n",
    "probabilidad de predicciones.\n",
    "\n",
    "<img src=\"img/stacking.jpg\">\n",
    "\n",
    "*Nota: El estimador de nivel 2 aquí se puede entrenar usando cualquier algoritmo de aprendizaje. historicamente, se han utilizado modelos lineales como la regresión lineal y la regresión logística.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacking with a second-level estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_stacking(level1_estimators, level2_estimator, X, y, use_probabilities=False):\n",
    "    fit(level1_estimators, X, y) # Trains level-1 base estimators\n",
    "    # Gets meta-features as individual predictions or prediction probabilities (proba=True/False)\n",
    "    X_meta = predict_individual(X, estimators=level1_estimators, proba=use_probabilities)\n",
    "    level2_estimator.fit(X_meta, y)\n",
    "    final_model = {'level-1': level1_estimators,\n",
    "                   'level-2': level2_estimator, # Saves the level-1 estimators and level-2 estimator in a dictionary\n",
    "                   'use-proba': use_probabilities}\n",
    "    return final_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making predictions with a stacked model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_stacking(X, stacked_model):\n",
    "    level1_estimators = stacked_model['level-1'] # Gets level-1 base estimators\n",
    "    use_probabilities = stacked_model['use-proba']\n",
    "    # Gets meta-features using the level-1 base estimators\n",
    "    X_meta = predict_individual(X, estimators=level1_estimators, proba=use_probabilities)\n",
    "    level2_estimator = stacked_model['level-2']\n",
    "    # Gets level-2 estimator and uses it to make the final predictions on the meta-features\n",
    "    y = level2_estimator.predict(X_meta) \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "meta_estimator = LogisticRegression(C=1.0, solver='lbfgs')\n",
    "\n",
    "stacking_model = fit_stacking(estimators, meta_estimator, Xtrn, ytrn, use_probabilities=True)\n",
    "\n",
    "ypred = predict_stacking(Xtst, stacking_model)\n",
    "\n",
    "tst_err = 1 - accuracy_score(ytst, ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Nota: El Stacking (Apilamiento) genera overfitting en los resultados, se puede mejorar esto utilizando k-fold cross validation en el entrenamiento*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacking with cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "def fit_stacking_with_CV(level1_estimators, level2_estimator, X, y, n_folds=5, use_probabilities=False):\n",
    "    n_samples = X.shape[0]\n",
    "    n_estimators = len(level1_estimators)\n",
    "    X_meta = np.zeros((n_samples, n_estimators)) # Initializes the metadata matrix\n",
    "    splitter = StratifiedKFold(n_splits=n_folds, shuffle=True)\n",
    "    # Trains level-1 estimators and then makes meta-features for the level-2 estimator with individual predictions\n",
    "    for trn, val in splitter.split(X, y):\n",
    "        level1_estimators = fit(level1_estimators, X[trn, :], y[trn])\n",
    "        X_meta[val, :] = predict_individual(X[val, :], estimators=level1_estimators, proba=use_probabilities)\n",
    "    level2_estimator.fit(X_meta, y)\n",
    "    level1_estimators = fit(level1_estimators, X, y)\n",
    "    final_model = {'level-1': level1_estimators,\n",
    "                   'level-2': level2_estimator, # Saves the level-1 estimators and level-2 estimator in a dictionary\n",
    "                   'use-proba': use_probabilities} \n",
    "    return final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "\n",
    "stacking_model = fit_stacking_with_CV(estimators, \n",
    "                                      meta_estimator, \n",
    "                                      Xtrn, \n",
    "                                      ytrn, \n",
    "                                      n_folds=5, \n",
    "                                      use_probabilities=True)\n",
    "\n",
    "ypred = predict_stacking(Xtst, stacking_model)\n",
    "\n",
    "tst_err = 1 - accuracy_score(ytst, ypred)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
