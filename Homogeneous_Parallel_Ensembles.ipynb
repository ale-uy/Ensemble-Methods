{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging with decision trees: training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "rng = np.random.RandomState(seed=4190)\n",
    "\n",
    "def bagging_fit(X, y, n_estimators, max_depth=5, max_samples=200): # Initialize a random seed\n",
    "    n_examples = len(y)\n",
    "    estimators = [DecisionTreeClassifier(max_depth=max_depth) # Creates a list of untrained base estimators\n",
    "    for _ in range(n_estimators)]\n",
    "    for tree in estimators:\n",
    "        bag = np.random.choice(n_examples, max_samples, # Generate a boostrap sample\n",
    "        replace=True)\n",
    "        tree.fit(X[bag, :], y[bag]) # Fits a tree to the boostrap sample\n",
    "    return estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging with decision trees: prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mode\n",
    "\n",
    "\n",
    "def bagging_predict(X, estimators):\n",
    "    # Predicts each test example using each estimator in the ensemble\n",
    "    all_predictions = np.array([tree.predict(X) for tree in estimators])\n",
    "    # Makes the final predictions by majority voting\n",
    "    ypred, _ = mode(all_predictions, axis=0, keepdims=False)\n",
    "    return np.squeeze(ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation EXAMPLE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "X, y = make_moons(n_samples=300, noise=.25, random_state=rng)\n",
    "\n",
    "Xtrn, Xtst, ytrn, ytst = train_test_split(X, y, test_size=0.33, random_state=rng)\n",
    "\n",
    "# Trains a bagging ensemble\n",
    "bag_ens = bagging_fit(Xtrn, ytrn, n_estimators=500, max_depth=12, max_samples=300)\n",
    "\n",
    "# Makes the final predictions by majority voting\n",
    "ypred = bagging_predict(Xtst, bag_ens)\n",
    "\n",
    "print(accuracy_score(ytst, ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging (with Scikit-learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "\n",
    "# Sets the base-learning algorithm along with hyperparameters\n",
    "base_estimator = DecisionTreeClassifier(max_depth=10)\n",
    "\n",
    "bag_ens = BaggingClassifier(base_estimator=base_estimator,\n",
    "                            n_estimators=500,   # Trains 500 base estimators\n",
    "                            max_samples=100,    # Each base estimator will be trained on a bootstrap sample of size 100.\n",
    "                            oob_score=True,     # Uses an OOB sample to estimate the generalization\n",
    "                            random_state=rng)   # Tip: n_jobs=–1 usa todos los cpus del pc\n",
    "\n",
    "bag_ens.fit(Xtrn, ytrn)\n",
    "ypred = bag_ens.predict(Xtst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TIP: El embolsado es más efectivo con clasificadores complejos y no lineales que\n",
    "tienden a sobreajustar los datos. Estos modelos complejos y sobreajustados son inestables, por lo que\n",
    "es decir, altamente sensible a pequeñas variaciones en los datos de entrenamiento. Para ver por qué, considere que los árboles de decisión individuales en un conjunto en bolsa tienen aproximadamente la misma complejidad. Sin embargo, debido al muestreo de arranque, se han entrenado en diferentes réplicas del conjunto de datos y sobreajustado de manera diferente. De otra manera, todos se sobreajustan aproximadamente en la misma cantidad, pero en diferentes lugares. El embolsado funciona mejor con tales modelos porque su agregación de modelos reduce el sobreajuste, lo que en última instancia conduce a un conjunto más robusto y estable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_ens.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(ytst, ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests (with scikit-learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "rf_ens = RandomForestClassifier(n_estimators=500,\n",
    "                                max_depth=10,\n",
    "                                oob_score=True,\n",
    "                                n_jobs=-1,\n",
    "                                random_state=rng)\n",
    "\n",
    "rf_ens.fit(Xtrn, ytrn)\n",
    "\n",
    "ypred = rf_ens.predict(Xtst) # Uses an OOB sample to estimate the generalization error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Seleccion:\n",
    "\n",
    "La selección de características, también conocida como selección de subconjuntos variables, es un procedimiento para identificar clasificando las características/atributos de datos más influyentes o relevantes. La selección de características es un paso importante del proceso de modelado, especialmente para datos de alta dimensión.\n",
    "Descartar las características menos relevantes a menudo mejora el rendimiento de la generalización y minimiza el sobreajuste. También suele mejorar la eficiencia computacional del entrenamiento.\n",
    "Estas preocupaciones son consecuencias de la maldición de la dimensionalidad, donde un gran número de características puede inhibir la capacidad de un modelo para generalizar de manera efectiva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain feature importances for the simple 2D data set\n",
    "for i, score in enumerate(rf_ens.feature_importances_):\n",
    "    print('Feature x{0}: {1:6.5f}'.format(i, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pasting: El embolsado (Bagging) utiliza muestreo de arranque (Boostrap) o muestreo con reemplazo. Si, en cambio, muestreamos subconjuntos para entrenamiento sin reemplazo, tenemos una variante de embolsado conocida como pegado (Pasting). El pegado se diseñó para conjuntos de datos muy grandes, donde no es necesario el muestreo con reemplazo\n",
    "\n",
    "Tip: BaggingClassifier se puede ampliar fácilmente para realizar el PASTING configurando bootstrap=False y convirtiéndolo en pequeños subconjuntos de submuestra para el entrenamiento configurando max_samples en una pequeña fracción, digamos max_samples=0.05."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Subspace and Random Patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/random_xx.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota: Random Subsapece y Random Patche NO usan necesariamente los arboles de decision, pueden usar cualquier algoritmo de ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo Extra Trees (Extreme Randomized Trees) es una variante del algoritmo de Random Forest en aprendizaje automático. Es un algoritmo de aprendizaje supervisado utilizado tanto para clasificación como para regresión.\n",
    "\n",
    "La principal característica distintiva de Extra Trees es que se aplica una estrategia de selección aleatoria más agresiva para construir los árboles de decisión en el bosque. Mientras que en Random Forest se seleccionan las mejores divisiones entre un subconjunto de características para cada nodo del árbol, en Extra Trees se eligen divisiones aleatorias sin tener en cuenta la importancia relativa de las características. Esto resulta en una mayor aleatoriedad y diversidad en los árboles construidos."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
